# -*- coding: utf-8 -*-
"""cust_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NBDjESGamzMDF1leLsE67azHYT4yCnOr
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler,normalize
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

creditcard_df = pd.read_csv("CC GENERAL.csv")

# CUSTID : Identification of Credit Card holder (Categorical)
# BALANCE : Balance amount left in their account to make purchases (
# BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)
# PURCHASES : Amount of purchases made from account
# ONEOFFPURCHASES : Maximum purchase amount done in one-go
# INSTALLMENTSPURCHASES : Amount of purchase done in installment
# CASHADVANCE : Cash in advance given by the user
# PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)
# ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)
# PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)
# CASHADVANCEFREQUENCY : How frequently the cash in advance being paid
# CASHADVANCETRX : Number of Transactions made with "Cash in Advanced"
# PURCHASESTRX : Number of purchase transactions made
# CREDITLIMIT : Limit of Credit Card for user
# PAYMENTS : Amount of Payment done by user
# MINIMUM_PAYMENTS : Minimum amount of payments made by user
# PRCFULLPAYMENT : Percent of full payment paid by user
# TENURE : Tenure of credit card service for user

creditcard_df

"""
### Visualization and exploration of dataset"""

#making a heatmap of null values; checking if there are any null values
sns.heatmap(creditcard_df.isnull(),yticklabels=False,cbar = False,cmap = "Blues")

#minimum_payments has some null values

creditcard_df.isnull().sum()

#filling up the missing elemets with mean of MINIMUM_PAYMENTS 
creditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True),'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()
creditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True),'CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].mean()

creditcard_df.isnull().sum() 
#now no more null values

#check for duplicated entries
creditcard_df.duplicated().sum()

#dropping the customerID column because it contains string
creditcard_df = creditcard_df.drop(['CUST_ID'], axis = 'columns')
#creditcard_df.drop(['CUST_ID'], axis = 1,inplace = True)

creditcard_df.describe()

"""### HeatMap"""

corr = creditcard_df.corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr,cmap="YlGnBu",annot=True) # Balance_freq, last 3

creditcard_df = creditcard_df.drop(['BALANCE_FREQUENCY'], axis = 'columns')
creditcard_df = creditcard_df.drop(['MINIMUM_PAYMENTS'], axis = 'columns')
creditcard_df = creditcard_df.drop(['PRC_FULL_PAYMENT'], axis = 'columns')
creditcard_df = creditcard_df.drop(['TENURE'], axis = 'columns')

creditcard_df.shape

len(creditcard_df.columns)

creditcard_df.columns

# distplot combines the matplotlib.hist function with seaborn kdeplot
# KDE Plot represents the Kernel Density Estimate
# KDE is used for visualization Probablity Density of a continuos variable
# KDE demonstrates the probablity density at different values in a continuous variable

plt.figure(figsize = (8,40))
for i in range(len(creditcard_df.columns)):
  plt.subplot(17,1,i+1) #subplot(nrows, ncols, index)
  sns.distplot(creditcard_df[creditcard_df.columns[i]],kde_kws = {"color":"b","lw":3,"label":"KDE"},hist_kws={"color":"r"})
  plt.title(creditcard_df.columns[i])

plt.tight_layout() #The tight_layout() function in pyplot module of matplotlib library is used to automatically adjust subplot parameters to give specified padding

#distplot to check distribution of data

#scaling the data first
scaler = StandardScaler()
creditcard_df_scaled = scaler.fit_transform(creditcard_df)

creditcard_df_scaled.shape

creditcard_df

creditcard_df_scaled

"""## Find the optimal number of clusters using elbow method"""

wss = [] 
cluster_range = range(1,24)

for i in cluster_range:
  km = KMeans(n_clusters = i)
  km.fit(creditcard_df) #fitting our data considering i no of clusters
  inertia = km.inertia_ #after fitting the model,inertia gives me sum of squres of distances
  wss.append(inertia) #appending to the empty list

print(wss)

plt.xlabel('k (number of clusters)')
plt.ylabel('Sum of squarred error/distances')
plt.plot(cluster_range,wss)
plt.show()

"""### Apply K-Means method"""

kmeans = KMeans(9)
kmeans.fit(creditcard_df_scaled)
labels = kmeans.labels_

kmeans.cluster_centers_

kmeans.cluster_centers_.shape

#converted the cluster centers to a Pandas DataFrame
cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_,columns = [creditcard_df.columns])
cluster_centers

#cluster_centers
#8 cluster, each cluster has 13 coordinates

#in order to understand what these numbers mean,let's perform inverse transformation
cluster_centers = scaler.inverse_transform(cluster_centers)

#lets put it into a datframe to see clearly
cluster_centers = pd.DataFrame(data = cluster_centers,columns=[creditcard_df.columns])
cluster_centers

labels.shape #these many data points will be categorized in 0-8 cluster_centers

len(cluster_centers)

labels.max()

labels.min()

y_kmeans = kmeans.predict(creditcard_df_scaled)
y_kmeans
#prediction of the classes, the customers belong to

#one more column we can create i.e the cluster assigned to each customer entry
creditcard_df_cluster = pd.concat([creditcard_df,pd.DataFrame({'cluster':labels})],axis = 1)
creditcard_df_cluster

#Plot the histogram of various clusters
for i in creditcard_df.columns:
  plt.figure(figsize = (30,5))
  for j in range(len(cluster_centers)):
    plt.subplot(1,9,j+1)
    cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]
    cluster[i].hist(bins = 10)
    plt.title('{}   \nCluster {} '. format(i,j))
plt.show()

"""## too many features, we dont want to work with all the features
## we cant visualize more than 3 dimensions
#Applying PCA(Principal component analysis)
"""

#kind of unsupervised algorithm - statistical tool
#obtain the principle components
pca = PCA(n_components = 2) # 2 principle components
principle_comp = pca.fit_transform(creditcard_df_scaled)
principle_comp

#creates 2 components from 17 features ->becoz we can't visualize 17 dimensions

pca_df = pd.DataFrame(data = principle_comp,columns = ['pca1','pca2'])
pca_df

#concat cluster labels
pca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})],axis = 1)
pca_df

"""### Plot Visualization"""

#we cant plot 3 D, is not supported by pathcollection

plt.figure(figsize = (10,10))
ax = sns.scatterplot(x = 'pca1',y='pca2',hue = "cluster",data = pca_df,palette = ['red','green','orange','black','yellow','maroon','orange','blue','magenta'])
#representing all 8 clusters

import pickle
# open a file, where you ant to store the data
file = open('cust_segmentation.pkl', 'wb')

# dump information to that file
pickle.dump(pca_df, file)

